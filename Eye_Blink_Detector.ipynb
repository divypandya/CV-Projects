{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eye Blink Detector.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divypandya/OpenCV-Projects/blob/master/Eye_Blink_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo6q48nX9pHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from scipy.spatial import distance as dist\n",
        "from imutils.video import FileVideoStream\n",
        "from imutils.video import VideoStream\n",
        "from imutils import face_utils\n",
        "import numpy as np\n",
        "import imutils\n",
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMnbrzuj6MCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eye_aspect_ratio(eye):\n",
        "    # compute the euclidean distances between the two sets of\n",
        "\t# vertical eye landmarks (x, y)-coordinates\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    \n",
        "    # compute the euclidean distance between the horizontal\n",
        "\t# eye landmark (x, y)-coordinates\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "    \n",
        "    EAR = (A + B)/(2.0 * C)\n",
        "    \n",
        "    return EAR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYNZGiC-_vRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {'shape predictor' :'drive/My Drive/Colab Notebooks/CV Tutorials/Face Detector/shape_predictor_68_face_landmarks.dat',\n",
        "       'video' : 'drive/My Drive/Colab Notebooks/CV Tutorials/Dataset/blink_detection_demo.mp4'\n",
        "       }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKHDz6EXAd16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define two constants, one for the eye aspect ratio to indicate\n",
        "# blink and then a second constant for the number of consecutive\n",
        "# frames the eye must be below the threshold\n",
        "EYE_AR_THRESH = 0.3\n",
        "EYE_AR_CONSEC_FRAMES = 3\n",
        "\n",
        "# initialize the frame counters and the total number of blinks\n",
        "counter = 0\n",
        "total = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLj59agCJ34F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1615e00a-3c69-4d81-98df-317a1421acb1"
      },
      "source": [
        "# initialize dlib's face detector (HOG-based) and then create\n",
        "# the facial landmark predictor\n",
        "print(\"[INFO] loading facial landmark predictor...\")\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(args['shape predictor'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading facial landmark predictor...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_3yaVx2LKMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grab the indexes of the facial landmarks for the left and\n",
        "# right eye, respectively\n",
        "(lstart, lend) = face_utils.FACIAL_LANDMARKS_IDXS['left_eye']\n",
        "(rstart, rend) = face_utils.FACIAL_LANDMARKS_IDXS['right_eye']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbixiTk8Lhe0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0636797d-dad4-437d-abfe-cf585d5a0464"
      },
      "source": [
        "# start the video stream thread\n",
        "print(\"[INFO] starting video stream thread...\")\n",
        "vs = FileVideoStream(args['video']).start()\n",
        "filestream = True\n",
        "# vs = VideoStream(src=0).start()\n",
        "# vs = VideoStream(usePiCamera=True).start()\n",
        "# fileStream = False\n",
        "time.sleep(1.0)\n",
        "writer = None\n",
        "Video_Stream_Out = 'drive/My Drive/Blink Detection.mp4'"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] starting video stream thread...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGSpU2lONHZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f2c64582-82fa-4920-8f8e-4e1d3ac359a4"
      },
      "source": [
        "# loop over frames from the video stream\n",
        "while True:\n",
        "    # if this is a file video stream, then we need to check if\n",
        "\t# there any more frames left in the buffer to process\n",
        "    if filestream and not vs.more():\n",
        "        break\n",
        "        \n",
        "    # grab the frame from the threaded video file stream, resize\n",
        "\t# it, and convert it to grayscale\n",
        "\t# channels\n",
        "    frame = vs.read()\n",
        "    frame = imutils.resize(frame, width = 450)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # detect faces in the grayscale frame\n",
        "    rects = detector(gray, 0)\n",
        "    \n",
        "    # loop over the face detections\n",
        "    for rect in rects:\n",
        "        # determine the facial landmarks for the face region, then\n",
        "\t\t# convert the facial landmark (x, y)-coordinates to a NumPy\n",
        "\t\t# array\n",
        "        shape = predictor(gray, rect)\n",
        "        shape = face_utils.shape_to_np(shape)\n",
        "        \n",
        "        # extract the left and right eye coordinates, then use the\n",
        "\t\t# coordinates to compute the eye aspect ratio for both eyes\n",
        "        lefteye = shape[lstart : lend]\n",
        "        righteye = shape[rstart : rend]\n",
        "        leftEAR = eye_aspect_ratio(lefteye)\n",
        "        rightEAR = eye_aspect_ratio(righteye)\n",
        "        \n",
        "        # average the eye aspect ratio together for both eyes\n",
        "        ear = (leftEAR + rightEAR)/2.0\n",
        "        \n",
        "        # compute the convex hull for the left and right eye, then\n",
        "\t\t# visualize each of the eyes\n",
        "        leftEyeHull = cv2.convexHull(lefteye)\n",
        "        rightEyeHull = cv2.convexHull(righteye)\n",
        "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
        "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
        "        \n",
        "        # check to see if the eye aspect ratio is below the blink\n",
        "\t\t# threshold, and if so, increment the blink frame counter\n",
        "        if ear < EYE_AR_THRESH:\n",
        "            counter += 1\n",
        "            \n",
        "        # otherwise, the eye aspect ratio is not below the blink\n",
        "\t\t# threshold\n",
        "        else:\n",
        "            # if the eyes were closed for a sufficient number of\n",
        "\t\t\t# then increment the total number of blinks\n",
        "            if counter > EYE_AR_CONSEC_FRAMES:\n",
        "                total += 1\n",
        "            \n",
        "            # reset the eye frame counter\n",
        "            counter = 0\n",
        "            \n",
        "        # draw the total number of blinks on the frame along with\n",
        "\t\t# the computed eye aspect ratio for the frame\n",
        "        cv2.putText(frame, 'BLINKS: {}'.format(total), (10, 20),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2\n",
        "                   )\n",
        "        \n",
        "        cv2.putText(frame, 'EAR: {}'.format(ear), (300, 30),\n",
        "                   cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 0, 255), 2\n",
        "                   )\n",
        "        \n",
        "        \n",
        "    # Check if the video writer is None\n",
        "    if writer is None:\n",
        "         # Initialize our video writer\n",
        "        fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
        "        writer = cv2.VideoWriter(Video_Stream_Out, fourcc, 30,\n",
        "        (frame.shape[1], frame.shape[0]), True)\n",
        "    \n",
        "    # Write the output frame to disk\n",
        "    writer.write(frame)\n",
        "    \n",
        "# Release the file pointers\n",
        "print(\"[INFO] cleaning up...\")\n",
        "writer.release()\n",
        "cv2.destroyAllWindows()\n",
        "        \n",
        "        "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] cleaning up...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}